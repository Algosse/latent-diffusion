{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      },
      "source": [
        "# Sequence restoration with Latent Diffusion Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "#%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms.functional import resize\n",
        "\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThxmCePqt1mt"
      },
      "source": [
        "Let's also check what type of GPU we've got."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "ROOT_PATH = \"C:/Users/Shadow/Documents/ImSeqCond/\"\n",
        "#os.chdir(\"/home/alban/ImSeqCond/latent-diffusion/\")\n",
        "os.chdir(os.path.join(ROOT_PATH, \"latent-diffusion/\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWAqdwk0Nrn"
      },
      "source": [
        "Load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fnGwQRhtyBhb"
      },
      "outputs": [],
      "source": [
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt=None):\n",
        "    model = instantiate_from_config(config.model)\n",
        "    \n",
        "    if ckpt is not None:\n",
        "        print(f\"Loading model from {ckpt}\")\n",
        "        pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
        "        sd = pl_sd[\"state_dict\"]\n",
        "        m, u = model.load_state_dict(sd, strict=False)\n",
        "    else:\n",
        "        print(\"Instantiated model from config\")\n",
        "        \n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "cond_key = 'label'\n",
        "#cond_key = 'LR_image'\n",
        "\n",
        "model_folder = os.path.join(ROOT_PATH, \"latent-diffusion/logs_saved/2024-01-18T22-34-24_config_siar_recon\")\n",
        "#model_folder = \"/home/alban/ImSeqCond/latent-diffusion/logs_saved/2023-12-21T15-15-42_config_siar_recon\"\n",
        "checkpoint = \"epoch=000048.ckpt\"\n",
        "\n",
        "files = os.listdir(os.path.join(model_folder, \"configs\"))\n",
        "config_file = \"\"\n",
        "for file in files:\n",
        "    if file.endswith(\"project.yaml\"):\n",
        "        config_file = file\n",
        "        break\n",
        "\n",
        "if config_file == \"\":\n",
        "    raise ValueError(\"No config file found\")\n",
        "\n",
        "def get_model(model_folder, config_file, checkpoint):\n",
        "    config = OmegaConf.load(os.path.join(model_folder, 'configs', config_file))\n",
        "    model = load_model_from_config(config, os.path.join(model_folder, \"checkpoints\", checkpoint))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnyd-XUKbfE",
        "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954"
      },
      "outputs": [],
      "source": [
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "\n",
        "model = get_model(model_folder, config_file, checkpoint)\n",
        "sampler = DDIMSampler(model)\n",
        "\n",
        "# count model parameters\n",
        "params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "print(f\"Model has {params/1e6:.2f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ldm'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load some custom data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msiar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SIAR\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m SIAR(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/SIAR\u001b[39m\u001b[38;5;124m\"\u001b[39m), set_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, max_sequence_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m#, downscale_f=4)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ldm'"
          ]
        }
      ],
      "source": [
        "# Load some custom data\n",
        "from ldm.data.siar import SIAR\n",
        "\n",
        "dataset = SIAR(os.path.join(ROOT_PATH, \"data/SIAR\"), set_type='val', resolution=256, max_sequence_size=10) #, downscale_f=4)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the index of an image in the dataset\n",
        "\n",
        "\"\"\" for i in range(len(dataset)):\n",
        "    if dataset[i]['name'] == \"7139\":\n",
        "        print(i)\n",
        "        break \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIEAhY8AhUrh"
      },
      "source": [
        "And go. Quality, sampling speed and diversity are best controlled via the `scale`, `ddim_steps` and `ddim_eta` variables. As a rule of thumb, higher values of `scale` produce better samples at the cost of a reduced output diversity. Furthermore, increasing `ddim_steps` generally also gives higher quality samples, but returns are diminishing for values > 250. Fast sampling (i e. low values of `ddim_steps`) while retaining good quality can be achieved by using `ddim_eta = 0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jcbqWX2Ytu9t",
        "outputId": "3b7adde0-d80e-4c01-82d2-bf988aee7455"
      },
      "outputs": [],
      "source": [
        "i = 10\n",
        "\n",
        "images_indexes = [i]\n",
        "n_samples_per_image = 6\n",
        "\n",
        "ddim_steps = 20\n",
        "ddim_eta = 1.0\n",
        "scale = 1# for unconditional guidance\n",
        "\n",
        "\n",
        "all_samples = list()\n",
        "\n",
        "with torch.no_grad():\n",
        "    with model.ema_scope():\n",
        "        \n",
        "        \"\"\" uc = model.get_learned_conditioning(\n",
        "            {model.cond_stage_key: torch.zeros(n_samples_per_image, 3, 64, 64).cuda().to(model.device)}\n",
        "            ) \"\"\"\n",
        "        \n",
        "        uc = model.get_learned_conditioning(\n",
        "            torch.zeros(n_samples_per_image, 3, 4, 256, 256).cuda().to(model.device)\n",
        "            )\n",
        "\n",
        "        for image_index in images_indexes:\n",
        "            print(f\"rendering {n_samples_per_image} examples of images '{dataset[image_index]['name']}' in {ddim_steps} steps and using s={scale:.2f}.\")\n",
        "            \n",
        "            if cond_key == 'LR_image':\n",
        "                xc = rearrange(torch.tensor(dataset[image_index]['LR_image']), 'h w c -> c h w').unsqueeze(0).repeat(n_samples_per_image, 1, 1, 1)\n",
        "            elif cond_key == 'label':\n",
        "                xc = rearrange(torch.tensor(dataset[image_index][cond_key]), 's h w c -> c s h w').unsqueeze(0).repeat(n_samples_per_image, 1, 1, 1, 1)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown cond_key '{cond_key}'\")\n",
        "\n",
        "            c = model.get_learned_conditioning(xc.to(model.device))\n",
        "\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                             conditioning=c,\n",
        "                                             batch_size=n_samples_per_image,\n",
        "                                             shape=[3, 64, 64],\n",
        "                                             verbose=False,\n",
        "                                             unconditional_guidance_scale=scale,\n",
        "                                             unconditional_conditioning=uc,\n",
        "                                             eta=ddim_eta)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
        "                                         min=0.0, max=1.0)\n",
        "            all_samples.append(x_samples_ddim)\n",
        " \n",
        "\n",
        "# display as grid\n",
        "grid = torch.stack(all_samples, 0)\n",
        "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "grid = make_grid(grid, nrow=n_samples_per_image)\n",
        "\n",
        "# to image\n",
        "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "Image.fromarray(grid.astype(np.uint8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_image(data, predict=None):\n",
        "    \"\"\" For a single data point, plot the ground truth image, the input images and the predicted image\n",
        "    Args:\n",
        "        gt (torch.tensor): ground truth image\n",
        "        input (torch.tensor): input images\n",
        "        predict (torch.tensor): predicted image\n",
        "    \"\"\"\n",
        "    gt, input = data['data'], data['label']\n",
        "    \n",
        "    label_images = data['label'].shape[0]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 6, figsize=(20, 10))\n",
        "\n",
        "    axes[0, 0].imshow(gt)\n",
        "    axes[0, 0].set_title(\"Ground truth\")\n",
        "    \n",
        "    for i in range(label_images):\n",
        "        axes[i//5, i%5 + 1].imshow(input[i])\n",
        "        axes[i//5, i%5 + 1].set_title(\"Input \" + str(i+1))\n",
        "        \n",
        "    if predict is not None:\n",
        "        axes[1, 0].imshow(predict)\n",
        "        axes[1, 0].set_title(\"Predicted\")\n",
        "        \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_for_plot(data, all_samples=None):\n",
        "    \n",
        "    data_prepared = dict()\n",
        "    for key, value in data.items():\n",
        "        if key in ['data', 'label']:\n",
        "            data_prepared[key] = (value + 1) / 2\n",
        "    \n",
        "    predict_prepared = rearrange(all_samples[0][0], 'c h w -> h w c')\n",
        "    #predict_prepared = (predict_prepared + 1) / 2\n",
        "    predict_prepared = predict_prepared.cpu().detach().numpy()\n",
        "    \n",
        "    return data_prepared, predict_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_image(*prepare_for_plot(dataset[i], all_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STUDY OF THE LATENT SPACE\n",
        "\n",
        "\"\"\" cond = c[0]\n",
        "\n",
        "# convert cond in 0, 1\n",
        "cond = (cond - cond.min()) / (cond.max() - cond.min())\n",
        "cond = rearrange(cond, 'c h w -> h w c')\n",
        "cond = cond.detach().cpu().numpy()\n",
        "\n",
        "cond_decode = model.decode_first_stage(c[0].unsqueeze(0))\n",
        "\n",
        "cond_decode = torch.clamp((cond_decode+1.0)/2.0,\n",
        "                                        min=0.0, max=1.0)\n",
        "cond_decode = rearrange(cond_decode.squeeze(), 'c h w -> h w c')\n",
        "cond_decode = cond_decode.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "axes[0].imshow(cond)\n",
        "axes[0].set_title(\"Cond in latent space\")\n",
        "\n",
        "axes[1].imshow(cond_decode)\n",
        "axes[1].set_title(\"Cond in pixel space\") \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark import Benchmark\n",
        "\n",
        "class BenchmarkLDM(Benchmark):\n",
        "    \n",
        "    def __init__(self, model, dataloader, mse=True, clip=False, lpips=False, cond_key='label'):\n",
        "        super().__init__(model, dataloader, mse, clip, lpips, cond_key)\n",
        "    \n",
        "    def sample(self, data, ddim_steps=20, ddim_eta=1.0, scale=1):\n",
        "        \"\"\" Method used to sample from the model with the data as conditionning \n",
        "            Args:\n",
        "                data (torch.tensor): conditionning data. size: (batch_size, 3, W, H) or (batch_size, 10, 3, W, H)\n",
        "            Output:\n",
        "                torch.tensor: restored image. size: (batch_size, 3, W, H)\n",
        "        \"\"\"\n",
        "\n",
        "        uc = self.model.get_learned_conditioning(\n",
        "            torch.zeros(data.shape[0], 3, 4, 256, 256).cuda().to(self.model.device)\n",
        "            )\n",
        "        \n",
        "        if self.cond_key == 'LR_image':\n",
        "            xc = rearrange(torch.tensor(data), 'b h w c -> b c h w')\n",
        "        elif self.cond_key == 'label':\n",
        "            xc = rearrange(torch.tensor(data), 'b s h w c -> b c s h w')\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown cond_key '{cond_key}'\")\n",
        "\n",
        "        c = self.model.get_learned_conditioning(xc.to(self.model.device))\n",
        "\n",
        "        samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                            conditioning=c,\n",
        "                                            batch_size=data.shape[0],\n",
        "                                            shape=[3, 64, 64],\n",
        "                                            verbose=False,\n",
        "                                            unconditional_guidance_scale=scale,\n",
        "                                            unconditional_conditioning=uc,\n",
        "                                            eta=ddim_eta)\n",
        "\n",
        "        x_samples_ddim = self.model.decode_first_stage(samples_ddim)\n",
        "        x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
        "                                        min=0.0, max=1.0)\n",
        "    \n",
        "        return x_samples_ddim\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark = BenchmarkLDM(model, dataloader, mse=True, clip=True, lpips=True, cond_key=cond_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = benchmark.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(benchmark.results)\n",
        "\n",
        "import json\n",
        "\n",
        "print(json.dumps(benchmark.results, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rescale(data):\n",
        "    \"\"\" Rescale data between 0 and 1 from -1 and 1\n",
        "        Args:\n",
        "            data (torch.tensor): data to rescale\n",
        "        Output:\n",
        "            torch.tensor: rescaled data\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'data': (data['data'] + 1) / 2,\n",
        "        'label': (data['label'] + 1) / 2,\n",
        "        'name': data['name'],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GENERATE SAMPLES\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "output_folder = os.path.join(model_folder, 'test_predictions_random')\n",
        "\n",
        "print(output_folder)\n",
        "\n",
        "for i in range(min(40, len(dataset))):\n",
        "    \n",
        "    j = np.random.randint(len(dataset))\n",
        "    data = dataset[j]\n",
        "    \n",
        "    y = data[cond_key]\n",
        "\n",
        "    predict = benchmark.sample(y[None,...],20)\n",
        "    #predict = model.predict(y.unsqueeze(0).to(device))\n",
        "    out = predict.detach().cpu()\n",
        "    \n",
        "    out = out[0].transpose(0,1).transpose(1,2)\n",
        "    \n",
        "    if os.path.exists(output_folder) == False:\n",
        "        os.makedirs(output_folder)\n",
        "        \n",
        "    # rescale data\n",
        "    data = rescale(data) # scale between 0 and 1\n",
        "    \n",
        "    plot_image(data, out)\n",
        "    \n",
        "    out_im = (out.numpy()* 255).astype(np.uint8) # rescale to 0-255\n",
        "    \n",
        "    im_pil = Image.fromarray(out_im)\n",
        "    im_pil.save(os.path.join(output_folder, f'{data[\"name\"]}.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from benchmark import Benchmark\n",
        "import os\n",
        "import json\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "    \"\"\" \n",
        "        Naive approach to solve the restoration problem\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Baseline, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\" \n",
        "            Forward pass\n",
        "            Args:\n",
        "                x (torch.tensor): input image sequence. size: (batch_size, 10, 3, 256, 256)\n",
        "            Returns:\n",
        "                torch.tensor: restored image\n",
        "        \"\"\"\n",
        "        out = torch.mean(x, dim=1)\n",
        "        \n",
        "        return out.permute(0, 3, 1, 2)\n",
        "    \n",
        "class BaselineBenchmark(Benchmark):\n",
        "    \n",
        "    def __init__(self, model, dataloader, mse=True, clip=False, lpips=False, cond_key='label'):\n",
        "        super().__init__(model, dataloader, mse, clip, lpips, cond_key)\n",
        "        \n",
        "    def sample(self, data):\n",
        "        \"\"\" Overwrite this method to sample from the model with data as conditioning \"\"\"\n",
        "        \n",
        "        data = data.to(device)\n",
        "        \n",
        "        return self.model(data)\n",
        "\n",
        "    def sample_repeat(self, data, n_rep=4):\n",
        "        \n",
        "        samples = []\n",
        "\n",
        "        for i in range(n_rep):\n",
        "            samples.append(self.sample(data))\n",
        "        \n",
        "        return rearrange(torch.stack(samples), 'n b c h w -> b n c h w')\n",
        "\n",
        "import torch\n",
        "from torchvision.transforms import transforms\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "class SIAR(torch.utils.data.Dataset):\n",
        "    \n",
        "    \"\"\"\n",
        "    Dataset class for SIAR dataset\n",
        "    \n",
        "    The dataset is constitued of N folders, N is the number of images of the dataset\n",
        "    Each folder contains 11 images, gt.png is the ground truth image, the others are distorted images (from 1 to 10)\n",
        "    \"\"\"\n",
        "    SPLIT_FILE = \"split.csv\"\n",
        "    WRONG_FILE = \"wrongs.txt\"\n",
        "    \n",
        "    def __init__(self, root, set_type='train', transform=None, generate_split=False, resolution=64, downscale_f=None, max_sequence_size=10, random_label=False):\n",
        "        \"\"\"\n",
        "        Args:   \n",
        "            path (str): path to the dataset\n",
        "            set (str): train, val or test\n",
        "            transform (torchvision.transforms): transform to apply to the images\n",
        "            generate_split (bool): if True, generate a new split file\n",
        "            resolution (int): resolution to resize the images to\n",
        "            downscale_f (int): if not None, downscale the ground truth to create a Lower Resolution image (LR_image)\n",
        "            max_sequence_size (int): maximum number of label images in a sequence\n",
        "            random_label (bool): if False, returns the first max_sequence_size images as label, \n",
        "                                 if True, returns a random sequence of [1, max_sequence_size] images as label\n",
        "        \"\"\"\n",
        "        \n",
        "        assert set_type in ['train', 'val', 'test'], \"set_type must be train, val or test\"\n",
        "        \n",
        "        self.root = root\n",
        "        \n",
        "        if not self._split_exists():\n",
        "            if generate_split:\n",
        "                self._generate_split()\n",
        "            else:\n",
        "                raise RuntimeError(\"Split file does not exist, please generate it with generate_split=True\")\n",
        "\n",
        "        self.wrong_images = self._get_wrong_images_list()\n",
        "\n",
        "        self.set_type = set_type\n",
        "        self.split = self._load_split()\n",
        "        self.images = self._load_images()\n",
        "        \n",
        "        self.len = len(self.images)\n",
        "\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.resolution = resolution\n",
        "        self.downscale_f = downscale_f\n",
        "        \n",
        "        self.max_sequence_size = max_sequence_size\n",
        "        self.random_label = random_label\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int or slice): index of item\n",
        "        returns:\n",
        "            dict: {'gt': gt, 'input': inputs} if single index\n",
        "            list: [{'gt': gt, 'input': inputs}, ...] if slice\n",
        "        \"\"\"\n",
        "        if isinstance(index, slice):\n",
        "            return [self._getitem(i) for i in range(*index.indices(self.len))]\n",
        "        else:\n",
        "            return self._getitem(index)\n",
        "        \n",
        "    def _getitem(self, index):\n",
        "        \"\"\" Get an item\n",
        "        Args:\n",
        "            index (int): index of item\n",
        "        Returns:\n",
        "            dict: {'gt': gt, 'input': inputs}\n",
        "                gt: np.array of shape (resolution, resolution, 3), ground truth image\n",
        "                label: np.array of shape (max_sequence_size, resolution, resolution, 3), input images\n",
        "        \"\"\"\n",
        "\n",
        "        if index >= self.len:\n",
        "            raise IndexError(\"Index out of range\")\n",
        "        if index < 0:\n",
        "            index += self.__len__()\n",
        "        \n",
        "        # read grund truth image\n",
        "        gt = Image.open(os.path.join(self.root, self.images[index], \"gt.png\"))\n",
        "        gt = gt.resize((self.resolution, self.resolution))\n",
        "        \n",
        "        # read input images\n",
        "        input = self.__get_label_images(index)\n",
        "        \n",
        "        # apply transform if any\n",
        "        if self.transform:\n",
        "            # CHANGES MADE THIS PART NOT\n",
        "            gt = self.transform(gt)\n",
        "            input = [self.transform(im) for im in input]\n",
        "            \n",
        "            input = torch.stack(input)\n",
        "            \n",
        "            if self.downscale_f is not None:\n",
        "                raise NotImplementedError(\"Downscale not implemented yet with transforms\")\n",
        "        else:\n",
        "            \"\"\" to_tensor = transforms.ToTensor()\n",
        "            gt = to_tensor(gt)\n",
        "            input = [to_tensor(im) for im in input]\n",
        "            \n",
        "            input = torch.stack(input) \"\"\"\n",
        "            \n",
        "            if self.downscale_f is not None:\n",
        "                lr_image = gt.resize((self.resolution // self.downscale_f, self.resolution // self.downscale_f))\n",
        "            else:\n",
        "                lr_image = gt\n",
        "            lr_image = np.array(lr_image).astype(np.uint8)\n",
        "            \n",
        "            gt = np.array(gt).astype(np.uint8)\n",
        "                        \n",
        "            if len(input) == 0:\n",
        "                input = gt\n",
        "            else:\n",
        "                input = np.stack(input)\n",
        "                \n",
        "                input = np.pad(input, ((0, self.max_sequence_size - input.shape[0]), (0, 0), (0, 0), (0, 0)), 'constant', constant_values=0)\n",
        "\n",
        "        return {\n",
        "            'data': (gt/127.5 - 1).astype(np.float32), \n",
        "            'label': (input/127.5 - 1).astype(np.float32),\n",
        "            'LR_image': (lr_image/127.5 - 1).astype(np.float32),\n",
        "            'name': self.images[index]            \n",
        "        }\n",
        "        \n",
        "    def __get_label_images(self, index):\n",
        "        \"\"\" Read the label images from the disk \n",
        "            if self.random_label is True, returns a random sequence of [1, max_sequence_size] images as label\n",
        "            else, returns the first max_sequence_size images as label   \n",
        "        \"\"\"\n",
        "        label = []\n",
        "        \n",
        "        if self.random_label:\n",
        "            label_size = random.randint(1, self.max_sequence_size)\n",
        "            indexes = random.sample(range(1, self.max_sequence_size + 1), label_size)\n",
        "        else:\n",
        "            indexes = range(1, self.max_sequence_size + 1)\n",
        "        \n",
        "        for i in indexes:\n",
        "            im = Image.open(os.path.join(self.root, self.images[index], str(i) + \".png\"))\n",
        "            im = im.resize((self.resolution, self.resolution))\n",
        "            label.append(np.array(im).astype(np.uint8))\n",
        "        \n",
        "        return label\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\" Get the length of the dataset \"\"\"\n",
        "        return self.len\n",
        "    \n",
        "    def _split_exists(self):\n",
        "        return os.path.exists(os.path.join(self.root, self.SPLIT_FILE))\n",
        "    \n",
        "    def _wrong_exists(self):\n",
        "        return os.path.exists(os.path.join(self.root, self.WRONG_FILE))\n",
        "    \n",
        "    def _generate_split(self):\n",
        "        \"\"\" Generate a split for the dataset \n",
        "            If the split already exists, please delete the file before generating a new one\n",
        "        \"\"\"\n",
        "        print(\"Generating split...\")\n",
        "        images = os.listdir(self.root)\n",
        "        \n",
        "        split = []\n",
        "        count = [0, 0, 0]\n",
        "    \n",
        "        for im in images:\n",
        "            r = random.random()\n",
        "            if r <= 0.8:\n",
        "                split.append(im + ',train')\n",
        "                count[0] += 1\n",
        "            elif r <= 0.9:\n",
        "                split.append(im + ',val')\n",
        "                count[1] += 1\n",
        "            else:\n",
        "                split.append(im + ',test')\n",
        "                count[2] += 1\n",
        "                \n",
        "        with open(os.path.join(self.root, self.SPLIT_FILE), 'w') as f:\n",
        "            f.write('\\n'.join(split))\n",
        "            \n",
        "        print(\"Split generated\")\n",
        "        print(\"Train: {}, Val: {}, Test: {}\".format(count[0], count[1], count[2]))\n",
        "\n",
        "    def _load_split(self):\n",
        "        \"\"\" Load the split file \"\"\"\n",
        "        \n",
        "        with open(os.path.join(self.root, self.SPLIT_FILE), 'r') as f:\n",
        "            data = f.read().split(\"\\n\")\n",
        "        \n",
        "        split = {}\n",
        "        for line in data:\n",
        "            if line == '':\n",
        "                continue\n",
        "            name, set_type = line.split(',')\n",
        "            split[name] = set_type\n",
        "            \n",
        "        return split\n",
        "    \n",
        "    def _load_images(self):\n",
        "        \"\"\" Load the images according to the split \"\"\"\n",
        "        im = []\n",
        "        \n",
        "        for name, set_type in self.split.items():\n",
        "            if set_type == self.set_type and name not in self.wrong_images:\n",
        "                im.append(name)\n",
        "                \n",
        "        return im\n",
        "    \n",
        "    def _get_wrong_images_list(self):\n",
        "        \"\"\" Some images are misslabelled in the dataset, this function returns the list of those images \"\"\"\n",
        "        \n",
        "        if self._wrong_exists():\n",
        "            with open(os.path.join(self.root, self.WRONG_FILE), 'r') as f:\n",
        "                data = f.read().split(\"\\n\")\n",
        "                print(\"Wrong images excluded\")\n",
        "                return data\n",
        "\n",
        "        print(\"No wrong images file found\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_folder = \"d:\\\\Cours\\\\Master_Arbeit\\\\code\\\\\"\n",
        "ROOT_PATH = \"d:\\\\Cours\\\\Master_Arbeit\\\\code\\\\\"\n",
        "model = Baseline()\n",
        "cond_key = 'label'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong images excluded\n",
            "Evaluating the model with sequence length 1\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3 [00:00<?, ?it/s]d:\\Cours\\Master_Arbeit\\code\\latent-diffusion\\scripts\\benchmark.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  im = torch.tensor(data['data']).permute(0, 3, 1, 2).to(device)\n",
            "100%|██████████| 3/3 [00:00<00:00, 152.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 0.7434548139572144, lowest score: 0.19783706963062286 for image 10, highest score: 1.2263375520706177 for image 10\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 2\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 78.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 0.873989462852478, lowest score: 0.2751019299030304 for image 10, highest score: 1.4087573289871216 for image 10\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 3\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 98.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 0.9964872598648071, lowest score: 0.4447501003742218 for image 10, highest score: 2.1954457759857178 for image 10034\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 4\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 114.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 0.9247682690620422, lowest score: 0.2682325839996338 for image 10, highest score: 1.810653805732727 for image 10034\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 5\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 90.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 1.0989768505096436, lowest score: 0.13379880785942078 for image 10, highest score: 2.4827871322631836 for image 10\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 6\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 59.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 1.2641005516052246, lowest score: 0.4447110891342163 for image 10034, highest score: 2.2697014808654785 for image 10\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 7\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 85.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 1.1160790920257568, lowest score: 0.29216137528419495 for image 10, highest score: 2.3366546630859375 for image 10\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 8\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 67.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 0.8373936414718628, lowest score: 0.20230530202388763 for image 10, highest score: 1.6802958250045776 for image 10034\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 9\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 59.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 1.3749839067459106, lowest score: 0.4753604829311371 for image 10034, highest score: 2.5789358615875244 for image 10\n",
            "Find results in 'results' attribute\n",
            "Wrong images excluded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model with sequence length 10\n",
            "Evaluating model on metrics mse\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 61.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metric mse score: 0.7398250699043274, lowest score: 0.13284368813037872 for image 10, highest score: 1.498531699180603 for image 10078\n",
            "Find results in 'results' attribute\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# generate samples with different sequence length\n",
        "max_images = 10\n",
        "\n",
        "#model = get_model(model_folder, config_file, checkpoint)\n",
        "\n",
        "output_folder = os.path.join(model_folder, 'test_predictions_sequence')\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for i in range(1, 11):\n",
        "    \n",
        "    dataset = SIAR(os.path.join(ROOT_PATH, \"data/SIAR\"), set_type='val', resolution=256, max_sequence_size=i, random_label=True)[:max_images] #, downscale_f=4)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)\n",
        "    \n",
        "    print(f\"Evaluating the model with sequence length {i}\")\n",
        "    \n",
        "    benchmark = BaselineBenchmark(model, dataloader, mse=True, clip=False, lpips=False, cond_key=cond_key)\n",
        "    \n",
        "    results = benchmark.evaluate()\n",
        "    \n",
        "    # save results as json\n",
        "    with open(os.path.join(output_folder, f\"results_{i}.json\"), 'w') as f:\n",
        "        json.dump(results, f)\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "latent-imagenet-diffusion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
