{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      },
      "source": [
        "# Sequence restoration with Latent Diffusion Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "#%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms.functional import resize\n",
        "\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThxmCePqt1mt"
      },
      "source": [
        "Let's also check what type of GPU we've got."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/home/alban/ImSeqCond/latent-diffusion/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWAqdwk0Nrn"
      },
      "source": [
        "Load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fnGwQRhtyBhb"
      },
      "outputs": [],
      "source": [
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt=None):\n",
        "    model = instantiate_from_config(config.model)\n",
        "    \n",
        "    if ckpt is not None:\n",
        "        print(f\"Loading model from {ckpt}\")\n",
        "        pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
        "        sd = pl_sd[\"state_dict\"]\n",
        "        m, u = model.load_state_dict(sd, strict=False)\n",
        "    else:\n",
        "        print(\"Instantiated model from config\")\n",
        "        \n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "#cond_key = 'label'\n",
        "cond_key = 'LR_image'\n",
        "\n",
        "model_folder = \"/home/alban/ImSeqCond/latent-diffusion/logs_saved/2023-12-21T00-11-09_config_siar_sr\"\n",
        "checkpoint = \"epoch=000036.ckpt\"\n",
        "\n",
        "files = os.listdir(os.path.join(model_folder, \"configs\"))\n",
        "config_file = \"\"\n",
        "for file in files:\n",
        "    if file.endswith(\"project.yaml\"):\n",
        "        config_file = file\n",
        "        break\n",
        "\n",
        "if config_file == \"\":\n",
        "    raise ValueError(\"No config file found\")\n",
        "\n",
        "def get_model():\n",
        "    config = OmegaConf.load(os.path.join(model_folder, 'configs', config_file))\n",
        "    model = load_model_from_config(config, os.path.join(model_folder, \"checkpoints\", checkpoint))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnyd-XUKbfE",
        "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954"
      },
      "outputs": [],
      "source": [
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "\n",
        "model = get_model()\n",
        "sampler = DDIMSampler(model)\n",
        "\n",
        "# count model parameters\n",
        "params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "print(f\"Model has {params/1e6:.2f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load some custom data\n",
        "from ldm.data.siar import SIAR\n",
        "\n",
        "dataset = SIAR(\"/home/alban/ImSeqCond/data/SIAR\", set_type='val', resolution=256, max_sequence_size=10, downscale_f=4)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIEAhY8AhUrh"
      },
      "source": [
        "And go. Quality, sampling speed and diversity are best controlled via the `scale`, `ddim_steps` and `ddim_eta` variables. As a rule of thumb, higher values of `scale` produce better samples at the cost of a reduced output diversity. Furthermore, increasing `ddim_steps` generally also gives higher quality samples, but returns are diminishing for values > 250. Fast sampling (i e. low values of `ddim_steps`) while retaining good quality can be achieved by using `ddim_eta = 0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jcbqWX2Ytu9t",
        "outputId": "3b7adde0-d80e-4c01-82d2-bf988aee7455"
      },
      "outputs": [],
      "source": [
        "i = 5\n",
        "\n",
        "images_indexes = [i]\n",
        "n_samples_per_image = 6\n",
        "\n",
        "ddim_steps = 20\n",
        "ddim_eta = 0.0\n",
        "scale = 3  # for unconditional guidance\n",
        "\n",
        "\n",
        "all_samples = list()\n",
        "\n",
        "with torch.no_grad():\n",
        "    with model.ema_scope():\n",
        "        \n",
        "        \"\"\" uc = model.get_learned_conditioning(\n",
        "            {model.cond_stage_key: torch.zeros(n_samples_per_image, 3, 64, 64).cuda().to(model.device)}\n",
        "            ) \"\"\"\n",
        "        \n",
        "        \"\"\" uc = model.get_learned_conditioning(\n",
        "            torch.zeros(n_samples_per_image, 3, 4, 64, 64).cuda().to(model.device)\n",
        "            ) \"\"\"\n",
        "\n",
        "        for image_index in images_indexes:\n",
        "            print(f\"rendering {n_samples_per_image} examples of images '{image_index}' in {ddim_steps} steps and using s={scale:.2f}.\")\n",
        "            \n",
        "            if cond_key == 'LR_image':\n",
        "                xc = rearrange(torch.tensor(dataset[image_index]['LR_image']), 'h w c -> c h w').unsqueeze(0).repeat(n_samples_per_image, 1, 1, 1)\n",
        "            elif cond_key == 'label':\n",
        "                xc = rearrange(torch.tensor(dataset[image_index][cond_key]), 's h w c -> c s h w').unsqueeze(0).repeat(n_samples_per_image, 1, 1, 1, 1)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown cond_key '{cond_key}'\")\n",
        "\n",
        "            c = model.get_learned_conditioning(xc.to(model.device))\n",
        "\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                             conditioning=c,\n",
        "                                             batch_size=n_samples_per_image,\n",
        "                                             shape=[3, 64, 64],\n",
        "                                             verbose=False,\n",
        "                                             unconditional_guidance_scale=scale,\n",
        "                                             #unconditional_conditioning=uc,\n",
        "                                             eta=ddim_eta)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
        "                                         min=0.0, max=1.0)\n",
        "            all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "# display as grid\n",
        "grid = torch.stack(all_samples, 0)\n",
        "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "grid = make_grid(grid, nrow=n_samples_per_image)\n",
        "\n",
        "# to image\n",
        "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "Image.fromarray(grid.astype(np.uint8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_image(data, predict=None):\n",
        "    \"\"\" For a single data point, plot the ground truth image, the input images and the predicted image\n",
        "    Args:\n",
        "        gt (torch.tensor): ground truth image\n",
        "        input (torch.tensor): input images\n",
        "        predict (torch.tensor): predicted image\n",
        "    \"\"\"\n",
        "    gt, input = data['data'], data['label']\n",
        "    \n",
        "    label_images = data['label'].shape[0]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 6, figsize=(20, 10))\n",
        "\n",
        "    axes[0, 0].imshow(gt)\n",
        "    axes[0, 0].set_title(\"Ground truth\")\n",
        "    \n",
        "    for i in range(label_images):\n",
        "        axes[i//5, i%5 + 1].imshow(input[i])\n",
        "        axes[i//5, i%5 + 1].set_title(\"Input \" + str(i+1))\n",
        "        \n",
        "    if predict is not None:\n",
        "        axes[1, 0].imshow(predict)\n",
        "        axes[1, 0].set_title(\"Predicted\")\n",
        "        \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_for_plot(data, all_samples=None):\n",
        "    \n",
        "    data_prepared = dict()\n",
        "    for key, value in data.items():\n",
        "        if key in ['data', 'label']:\n",
        "            data_prepared[key] = (value + 1) / 2\n",
        "    \n",
        "    predict_prepared = rearrange(all_samples[0][0], 'c h w -> h w c')\n",
        "    #predict_prepared = (predict_prepared + 1) / 2\n",
        "    predict_prepared = predict_prepared.cpu().detach().numpy()\n",
        "    \n",
        "    return data_prepared, predict_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_image(*prepare_for_plot(dataset[i], all_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STUDY OF THE LATENT SPACE\n",
        "\n",
        "\"\"\" cond = c[0]\n",
        "\n",
        "# convert cond in 0, 1\n",
        "cond = (cond - cond.min()) / (cond.max() - cond.min())\n",
        "cond = rearrange(cond, 'c h w -> h w c')\n",
        "cond = cond.detach().cpu().numpy()\n",
        "\n",
        "cond_decode = model.decode_first_stage(c[0].unsqueeze(0))\n",
        "\n",
        "cond_decode = torch.clamp((cond_decode+1.0)/2.0,\n",
        "                                        min=0.0, max=1.0)\n",
        "cond_decode = rearrange(cond_decode.squeeze(), 'c h w -> h w c')\n",
        "cond_decode = cond_decode.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "axes[0].imshow(cond)\n",
        "axes[0].set_title(\"Cond in latent space\")\n",
        "\n",
        "axes[1].imshow(cond_decode)\n",
        "axes[1].set_title(\"Cond in pixel space\") \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from benchmark import Benchmark\n",
        "\n",
        "class BenchmarkLDM(Benchmark):\n",
        "    \n",
        "    def __init__(self, model, dataloader, mse=True, clip=False, lpips=False, cond_key='label'):\n",
        "        super().__init__(model, dataloader, mse, clip, lpips, cond_key)\n",
        "    \n",
        "    def sample(self, data, ddim_steps=200, ddim_eta=0.0, scale=1):\n",
        "        \"\"\" Method used to sample from the model with the data as conditionning \n",
        "            Args:\n",
        "                data (torch.tensor): conditionning data. size: (batch_size, 3, W, H) or (batch_size, 10, 3, W, H)\n",
        "            Output:\n",
        "                torch.tensor: restored image. size: (batch_size, 3, W, H)\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.cond_key == 'LR_image':\n",
        "            xc = rearrange(torch.tensor(data), 'b h w c -> b c h w')\n",
        "        elif self.cond_key == 'label':\n",
        "            xc = rearrange(torch.tensor(data), 'b s h w c -> b c s h w')\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown cond_key '{cond_key}'\")\n",
        "\n",
        "        c = model.get_learned_conditioning(xc.to(model.device))\n",
        "\n",
        "        samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                            conditioning=c,\n",
        "                                            batch_size=data.shape[0],\n",
        "                                            shape=[3, 64, 64],\n",
        "                                            verbose=False,\n",
        "                                            unconditional_guidance_scale=scale,\n",
        "                                            eta=ddim_eta)\n",
        "\n",
        "        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "        x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
        "                                        min=0.0, max=1.0)\n",
        "    \n",
        "        return x_samples_ddim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark = BenchmarkLDM(model, dataloader, mse=True, clip=True, lpips=True, cond_key=cond_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#results = benchmark.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rescale(data):\n",
        "    \"\"\" Rescale data between 0 and 1 from -1 and 1\n",
        "        Args:\n",
        "            data (torch.tensor): data to rescale\n",
        "        Output:\n",
        "            torch.tensor: rescaled data\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'data': (data['data'] + 1) / 2,\n",
        "        'label': (data['label'] + 1) / 2,\n",
        "        'name': data['name'],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GENERATE SAMPLES\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "output_folder = os.path.join(model_folder, 'test_predictions_random')\n",
        "\n",
        "print(output_folder)\n",
        "\n",
        "for i in range(min(40, len(dataset))):\n",
        "    \n",
        "    j = np.random.randint(len(dataset))\n",
        "    data = dataset[j]\n",
        "    \n",
        "    y = data[cond_key]\n",
        "\n",
        "    predict = benchmark.sample(y[None,...],20)\n",
        "    #predict = model.predict(y.unsqueeze(0).to(device))\n",
        "    out = predict.detach().cpu()\n",
        "    \n",
        "    out = out[0].transpose(0,1).transpose(1,2)\n",
        "    \n",
        "    if os.path.exists(output_folder) == False:\n",
        "        os.makedirs(output_folder)\n",
        "        \n",
        "    # rescale data\n",
        "    data = rescale(data) # scale between 0 and 1\n",
        "    \n",
        "    plot_image(data, out)\n",
        "    \n",
        "    out_im = (out.numpy()* 255).astype(np.uint8) # rescale to 0-255\n",
        "    \n",
        "    im_pil = Image.fromarray(out_im)\n",
        "    im_pil.save(os.path.join(output_folder, f'{data[\"name\"]}.png'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "latent-imagenet-diffusion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
