{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUmmV5ZvrPbP"
      },
      "source": [
        "# Super resolution with Latent Diffusion Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThxmCePqt1mt"
      },
      "source": [
        "Let's also check what type of GPU we've got."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbL2zJ7Pt7Jl",
        "outputId": "c8242be9-dba2-4a9f-da44-a294a70bb449"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/home/alban/ImSeqCond/latent-diffusion/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWAqdwk0Nrn"
      },
      "source": [
        "Load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "fnGwQRhtyBhb"
      },
      "outputs": [],
      "source": [
        "#@title loading utils\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt=None):\n",
        "    model = instantiate_from_config(config.model)\n",
        "    \n",
        "    if ckpt is not None:\n",
        "        print(f\"Loading model from {ckpt}\")\n",
        "        pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
        "        sd = pl_sd[\"state_dict\"]\n",
        "        m, u = model.load_state_dict(sd, strict=False)\n",
        "    else:\n",
        "        print(\"Instantiated model from config\")\n",
        "        \n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    config = OmegaConf.load(\"/home/alban/ImSeqCond/latent-diffusion/logs/2023-12-21T15-15-42_config_siar_recon/configs/2023-12-21T15-15-42-project.yaml\")\n",
        "    model = load_model_from_config(config, \"/home/alban/ImSeqCond/latent-diffusion/logs/2023-12-21T15-15-42_config_siar_recon/checkpoints/epoch=000034.ckpt\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPnyd-XUKbfE",
        "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 113.62 M params.\n",
            "Keeping EMAs of 308.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Restored from models/first_stage_models/vq-f4/model.ckpt with 0 missing and 55 unexpected keys\n",
            "Loading model from /home/alban/ImSeqCond/latent-diffusion/logs/2023-12-21T15-15-42_config_siar_recon/checkpoints/epoch=000034.ckpt\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.92 GiB total capacity; 773.98 MiB already allocated; 12.38 MiB free; 788.00 MiB reserved in total by PyTorch)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDIMSampler\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m sampler \u001b[38;5;241m=\u001b[39m DDIMSampler(model)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# count model parameters\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m():\n\u001b[1;32m     25\u001b[0m     config \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/alban/ImSeqCond/latent-diffusion/logs/2023-12-21T15-15-42_config_siar_recon/configs/2023-12-21T15-15-42-project.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/alban/ImSeqCond/latent-diffusion/logs/2023-12-21T15-15-42_config_siar_recon/checkpoints/epoch=000034.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, ckpt)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ckpt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     pl_sd \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, map_location=\"cpu\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     sd \u001b[38;5;241m=\u001b[39m pl_sd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m     m, u \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(sd, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m~/ImSeqCond/.venv_ldm/lib/python3.8/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    593\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 594\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
            "File \u001b[0;32m~/ImSeqCond/.venv_ldm/lib/python3.8/site-packages/torch/serialization.py:853\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    851\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mUnpickler(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    852\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m--> 853\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/ImSeqCond/.venv_ldm/lib/python3.8/site-packages/torch/serialization.py:845\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    843\u001b[0m data_type, key, location, size \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m--> 845\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m storage \u001b[38;5;241m=\u001b[39m loaded_storages[key]\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m storage\n",
            "File \u001b[0;32m~/ImSeqCond/.venv_ldm/lib/python3.8/site-packages/torch/serialization.py:834\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    831\u001b[0m dtype \u001b[38;5;241m=\u001b[39m data_type(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    833\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, size, dtype)\u001b[38;5;241m.\u001b[39mstorage()\n\u001b[0;32m--> 834\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ImSeqCond/.venv_ldm/lib/python3.8/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/ImSeqCond/.venv_ldm/lib/python3.8/site-packages/torch/serialization.py:157\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m storage_type(obj\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ImSeqCond/.venv_ldm/lib/python3.8/site-packages/torch/_utils.py:79\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
            "File \u001b[0;32m~/ImSeqCond/.venv_ldm/lib/python3.8/site-packages/torch/cuda/__init__.py:462\u001b[0m, in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m _lazy_init()\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# We may need to call lazy init again if we are a forked child\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# del _CudaBase.__new__\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_CudaBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.92 GiB total capacity; 773.98 MiB already allocated; 12.38 MiB free; 788.00 MiB reserved in total by PyTorch)"
          ]
        }
      ],
      "source": [
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "\n",
        "model = get_model()\n",
        "sampler = DDIMSampler(model)\n",
        "\n",
        "# count model parameters\n",
        "params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "print(f\"Model has {params/1e6:.2f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load some custom data\n",
        "from ldm.data.siar import SIAR\n",
        "\n",
        "dataset = SIAR(\"/home/alban/ImSeqCond/data/SIARmini\", set_type='train', resolution=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIEAhY8AhUrh"
      },
      "source": [
        "And go. Quality, sampling speed and diversity are best controlled via the `scale`, `ddim_steps` and `ddim_eta` variables. As a rule of thumb, higher values of `scale` produce better samples at the cost of a reduced output diversity. Furthermore, increasing `ddim_steps` generally also gives higher quality samples, but returns are diminishing for values > 250. Fast sampling (i e. low values of `ddim_steps`) while retaining good quality can be achieved by using `ddim_eta = 0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jcbqWX2Ytu9t",
        "outputId": "3b7adde0-d80e-4c01-82d2-bf988aee7455"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms.functional import resize\n",
        "\n",
        "i = 1\n",
        "\n",
        "images_indexes = [i]\n",
        "n_samples_per_image = 6\n",
        "\n",
        "ddim_steps = 500\n",
        "ddim_eta = 0.0\n",
        "scale = 1   # for unconditional guidance\n",
        "\n",
        "\n",
        "all_samples = list()\n",
        "\n",
        "with torch.no_grad():\n",
        "    with model.ema_scope():\n",
        "        \n",
        "        uc = model.get_learned_conditioning(\n",
        "            {model.cond_stage_key: torch.zeros(n_samples_per_image, 3, 64, 64).cuda().to(model.device)}\n",
        "            )\n",
        "\n",
        "        for image_index in images_indexes:\n",
        "            print(f\"rendering {n_samples_per_image} examples of images '{image_index}' in {ddim_steps} steps and using s={scale:.2f}.\")\n",
        "            \n",
        "            resized = resize(rearrange(torch.tensor(dataset[image_index]['data']), 'h w c -> c h w'), (64, 64))\n",
        "            xc = resized.unsqueeze(0).repeat(n_samples_per_image, 1, 1, 1)\n",
        "            \n",
        "            c = model.get_learned_conditioning({model.cond_stage_key: xc.to(model.device)})\n",
        "\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                             conditioning=c[model.cond_stage_key],\n",
        "                                             batch_size=n_samples_per_image,\n",
        "                                             shape=[3, 64, 64],\n",
        "                                             verbose=False,\n",
        "                                             unconditional_guidance_scale=scale,\n",
        "                                             unconditional_conditioning=uc[model.cond_stage_key],\n",
        "                                             eta=ddim_eta)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
        "                                         min=0.0, max=1.0)\n",
        "            all_samples.append(x_samples_ddim)\n",
        "\n",
        "\n",
        "# display as grid\n",
        "grid = torch.stack(all_samples, 0)\n",
        "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "grid = make_grid(grid, nrow=n_samples_per_image)\n",
        "\n",
        "# to image\n",
        "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "Image.fromarray(grid.astype(np.uint8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original = (dataset[i]['data'] + 1) * 127.5\n",
        "\n",
        "Image.fromarray(original.astype(np.uint8))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "latent-imagenet-diffusion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
